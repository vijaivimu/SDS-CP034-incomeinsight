# ğŸ“„ IncomeInsight â€“ Project Report - ğŸŸ¢ **Beginner Track**

Welcome to your personal project report!  
Use this file to answer the key reflection questions for each phase of the project. This report is designed to help you think like a data scientist, guide AI tools more effectively, and prepare for real-world job interviews.

---

## âœ… Week 1: Setup & Exploratory Data Analysis (EDA)

> Answer the EDA questions provided in the project materials here. Focus on data quality, trends, anomalies, and relationships.

### ğŸ”‘ Question 1: What features show the strongest correlation with earning >$50K?


### ğŸ”¢ Numerical Correlation Analysis

I computed Pearson correlations between numerical features and the binary income label (`<=50K` = 0, `>50K` = 1). The following heatmap summarizes the results:

![Correlation Matrix](charts/question1_correlation_matrix.png)

**Key Observations**:
- `education-num`, `capital-gain`, and `hours-per-week` have the strongest positive correlation with income.
- `fnlwgt` and `capital-loss` have weaker or inconsistent correlations.

### ğŸ§  Mutual Information (All Features)

I computed mutual information to evaluate how informative each feature is for predicting income. This technique works with both categorical and numerical features.

![Mutual Information Scores](charts/question1_mutual_info.png)

**Top Predictors**:
- `relationship` and `marital.status` are the most informative features.
- `capital.gain`, `education`, and `occupation` also show strong predictive power.
- `race`, `native-country`, and `fnlwgt` contribute very little.

---

### ğŸ”‘ Question 2: How does income vary with education, marital status, or hours worked per week?

### ğŸ“ Education and Income

![Income by Education](charts/question2_income_by_education.png)

- Individuals with **Bachelorâ€™s, Masterâ€™s, and Doctorate degrees** are more likely to earn >$50K.
- The vast majority of individuals with **lower education levels (HS-grad and below)** earn â‰¤$50K.
- Advanced education is correlated with higher earning potential, though such individuals are fewer in the dataset.

### ğŸ’ Marital Status and Income

![Income by Marital Status](charts/question2_income_by_marital_status.png)

- Individuals who are **"Married-civ-spouse"** have the **highest proportion** of >$50K earners.
- **"Never-married"** individuals are predominantly in the â‰¤$50K category, suggesting lower income is common among single people.
- Categories such as **"Divorced"**, **"Separated"**, and **"Widowed"** show very low high-income representation.

### ğŸ•’ Hours Worked per Week and Income

![Hours Worked vs Income](charts/question2_income_by_hours_per_week.png)

- Both income groups have a **median of 40 hours/week**, as shown in the labels above each box.
- However, individuals earning >$50K display a **much wider range** of work hours.
- Many high-income earners work **significantly more than 40 hours**, suggesting that while 40 is a common standard, putting in extra hours is often associated with higher income.

### ğŸ”‘ Question 3: Are there disparities across race, sex, or native country?

### ğŸŒ Race and Income

![Income by Race](charts/question3_income_by_race.png)

- The majority of the dataset consists of White individuals, who also make up most of the high-income earners.
- Other racial groups â€” such as Black, Asian-Pac-Islander, Amer-Indian-Eskimo, and Other â€” are underrepresented in the >$50K income bracket.
- There are clear disparities in income distribution across racial categories, with White individuals having the highest likelihood of earning >$50K.

### ğŸ§‘â€ğŸ¤â€ğŸ§‘ Sex and Income

![Income by Sex](charts/question3_income_by_sex.png)

- **Men** are significantly more likely to earn >$50K compared to women.
- While both genders have a large presence in the dataset, the proportion of **high-income males** is much greater.
- The **gender income disparity** is clearly visible in this distribution.


### ğŸŒ Native Country and Income

![Income by Native Country](charts/question3_income_by_country_top10.png)

- The dataset is heavily skewed toward individuals from the **United States**, which dominates both income brackets.
- Non-U.S. countries show **very few individuals earning >$50K**, even in the top 10 most common native countries.
- This may indicate disparities in economic opportunities or representation among immigrants and foreign-born residents.
- For modeling purposes, this imbalance should be considered to avoid bias toward U.S.-centric outcomes.


### ğŸ”‘ Question 4: Do capital gains/losses strongly impact the income label?

### ğŸ’° Capital Gains and Income

![Capital Gain KDE (Log)](charts/question4_capital_gain_kde_log.png)

- Most individuals have little or no capital gains (visible as a high spike near 0).
- A **notable portion of high-income earners** have significant capital gains â€” evident from the **secondary peak** in the >$50K group between log(1 + gain) 8 to 10.
- This indicates that **capital gains strongly correlate with higher income**, making it a valuable predictive feature.


### ğŸ“‰ Capital Loss and Income

![Capital Loss KDE (Log)](charts/question4_capital_loss_kde_log.png)

- The majority of both income groups report **no capital losses**, seen in the peak at log(1) = 0.
- A **small subset of higher-income individuals** shows a secondary peak around log(1+loss) â‰ˆ 8, indicating losses of ~$3,000.
- While this suggests some relationship between **investment-related loss and high income**, the effect is **less pronounced than capital gains**.


## âœ… Week 2: Feature Engineering & Data Preprocessing

### ğŸ”‘ Question 1:
**Which features in the dataset appear to have the strongest relationship with the income label (>50K), and how did you determine this?**  

To identify which features most influence income, we calculated **Mutual Information (MI) scores**, a measure that captures the amount of information each feature contributes to predicting the target variable (`income`). This method accommodates both **categorical** and **numerical** features after encoding.

The results are shown below:

| Feature           | MI Score   |
|------------------|------------|
| relationship      | **0.1133** |
| marital.status    | **0.1111** |
| capital.gain      | 0.0822     |
| education.num     | 0.0666     |
| age               | 0.0662     |
| education         | 0.0642     |
| occupation        | 0.0617     |
| hours.per.week    | 0.0408     |
| capital.loss      | 0.0351     |
| sex               | 0.0314     |
| fnlwgt            | 0.0311     |
| workclass         | 0.0147     |
| race              | 0.0081     |
| native.country    | 0.0032     |

**Key Insight:**
- The highest mutual information scores are from `relationship` (0.1133), `marital.status` (0.1111), and `capital.gain` (0.0822), indicating that these features have the most **significant impact** on income prediction.
- Features with low MI scores, such as `race` and `native.country`, contribute relatively little to income prediction.

ğŸ“Š **Visual Summary:**  
![Mutual Information Bar Chart](charts/week2_q1_mutual_info.png)

---

### ğŸ”‘ Question 2:
**Did you engineer any new features from existing ones? If so, explain the new feature(s) and why you think they might help your classifier.**  

#### ğŸ”§ Engineered Features Analysis (Q2)

To enhance model interpretability, I created the following features:
- `occupation_mean_hours`: Avg. weekly hours per occupation
- `workclass_mean_hours`: Avg. weekly hours per workclass

I evaluated their usefulness via the correlation matrix below:

ğŸ“Š ![Heatmap Including Engineered Features](charts/week2_q2_heatmap_engineered_features.png)

**Observations:**
- `occupation_mean_hours` shows a meaningful positive correlation with income (~0.24), suggesting people in time-intensive occupations are more likely to earn >50K.
- `workclass_mean_hours` also shows a modest correlation (~0.14), which may contribute some value but is relatively weaker than `occupation_mean_hours`.


### ğŸ”‘ Question 3:
**Which continuous features required scaling or transformation before modeling, and which method did you use?**  

Several features had different value ranges or skewed distributions that required transformation or scaling for effective model training:

#### âœ… Features Scaled Using `StandardScaler`:
I applied standardization (zero mean, unit variance) to the following features that were roughly symmetric:

- `age`
- `education.num`
- `hours.per.week`
- `occupation_mean_hours`
- `workclass_mean_hours`

#### ğŸ” Features Transformed Using `log1p(x)`:
These features were highly right-skewed and contained many zero values, so we applied a log transformation before scaling:

- `capital.gain`
- `capital.loss`
- `fnlwgt` (optional but included)

This preprocessing ensures better convergence during training and reduces the impact of outliers or skewed distributions.

I then applied `StandardScaler` on the transformed values to keep all continuous features on a consistent scale.

---

### ğŸ”‘ Question 4:
**Is the target variable (`income`) imbalanced? How did you check, and what will you do (if anything) to handle it?**  

**Is the target variable (`income`) imbalanced? How did you check, and what will you do (if anything) to handle it?**

I checked the balance of the `income` target variable by calculating class proportions and plotting a bar chart:

ğŸ“Š **Income Class Distribution (%):**
- Class `0` (â‰¤50K): **75.1%**  
- Class `1` (>50K): **24.9%**

![Income Distribution](charts/week2_q4_income_distribution.png)

This indicates a significant class imbalance, where most individuals fall into the â‰¤50K category.

To address this imbalance, I plan to:
- Use **stratified sampling** when splitting the dataset into training and testing sets  
- Consider models that support **class weighting** (e.g., `class_weight='balanced'`)  
- Optionally try **oversampling techniques** like SMOTE if model performance is impacted

### ğŸ”‘ Question 5:
**What does your final cleaned dataset look like before modeling? Include shape, types of features (numerical/categorical), and a summary of the preprocessing steps applied.**  
ğŸ¯ *Purpose: Encourages documentation and preparation for modeling.*

ğŸ’¡ **Hint:**  
Use `df.shape`, `df.dtypes`, and summarize what was dropped, encoded, scaled, or engineered.

âœï¸ *Your answer here...*

---


---

### âœ… Week 3: Model Development & Experimentation

### ğŸ”‘ Question 1:
**Which classification models did you train for predicting income, and what are the strengths or assumptions of each model?**  
ğŸ¯ *Purpose: Tests understanding of algorithm selection and fit for the problem.*

ğŸ’¡ **Hint:**  
Train Logistic Regression (baseline, interpretable), Random Forest (handles non-linearities), and XGBoost (boosted performance).  
Explain what each model assumes (e.g., linearity in Logistic Regression) or does well (e.g., handling missing values, feature interactions).

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 2:
**How did each model perform based on your evaluation metrics (accuracy, precision, recall, F1-score, ROC-AUC)? Which performed best, and why?**  
ğŸ¯ *Purpose: Tests ability to evaluate and compare classifiers fairly.*

ğŸ’¡ **Hint:**  
Use `classification_report`, `confusion_matrix`, and `roc_auc_score`.  
Show results in a table or chart.  
Explain model strengths (e.g., better recall = catches more high-income earners).

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 3:
**Is your model biased toward one class (>$50K or â‰¤$50K)? How did you detect this, and what might you do to fix it?**  
ğŸ¯ *Purpose: Tests understanding of class imbalance and metric interpretation.*

ğŸ’¡ **Hint:**  
Inspect confusion matrix, precision/recall per class.  
Use `.value_counts()` on the `income` label to see imbalance.  
Consider using `class_weight='balanced'` or resampling techniques.

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 4:
**What features were most important in your best-performing model, and do they align with expectations about income prediction?**  
ğŸ¯ *Purpose: Tests interpretability and domain reasoning.*

ğŸ’¡ **Hint:**  
Use `.feature_importances_` for tree models or `.coef_` for Logistic Regression.  
Do features like `education`, `occupation`, or `hours_per_week` appear at the top?  
Visualize using bar plots.

âœï¸ *Your answer here...*

---

### ğŸ”‘ Question 5:
**How did you use MLflow to track your model experiments, and what comparisons did it help you make?**  
ğŸ¯ *Purpose: Tests reproducibility and experiment tracking skills.*

ğŸ’¡ **Hint:**  
Log model name, hyperparameters, evaluation metrics, and notes.  
Use MLflowâ€™s comparison view to track which run performed best.  
Share screenshots or describe insights gained.

âœï¸ *Your answer here...*

---

## âœ… Week 4: Model Selection & Hyperparameter Tuning

### ğŸ”‘ Question 1:
**Which hyperparameters did you tune for your best-performing model, and how did you decide which ones to adjust?**

ğŸ’¡ **Hint:**
For Logistic Regression: C, solver.
For Random Forest: n_estimators, max_depth, min_samples_split.
For XGBoost: learning_rate, max_depth, n_estimators.

âœï¸ Your answer here...

---

### ğŸ”‘ Question 2:
**What method did you use for hyperparameter tuning (e.g., GridSearchCV, RandomizedSearchCV), and why?**

ğŸ’¡ **Hint:**
GridSearch = exhaustive but slow.
RandomizedSearch = faster, good for large search spaces.

âœï¸  *Your answer here...*


---

### ğŸ”‘ Question 3:
**How did the tuned modelâ€™s performance compare to the baseline version, and what does that tell you about the value of tuning?**

ğŸ’¡ **Hint:**
Compare metrics (accuracy, F1, AUC) from Week 3 and Week 4 side-by-side.
Small improvements may still matter in real-world deployment.

âœï¸  *Your answer here...*

---

### ğŸ”‘ Question 4:
**What risk of overfitting did you observe during tuning, and how did you mitigate it?**

ğŸ’¡ **Hint:**
Use cross-validation and monitor gap between train/test metrics.
Apply early stopping (XGBoost), pruning (trees), or reduce model complexity.

âœï¸  *Your answer here...*

---

### ğŸ”‘ Question 5:

**Which model and hyperparameters did you ultimately select for deployment, and why?**

ğŸ’¡ **Hint:**
Explain the tradeoff between interpretability, performance, and generalization.
Justify final parameter choices and their business relevance.

âœï¸  *Your answer here...*

---

## âœ… Week 5: Model Deployment

> Document your approach to building and deploying the Streamlit app, including design decisions, deployment steps, and challenges.

### ğŸ”‘ Question 1:

### ğŸ”‘ Question 2:

### ğŸ”‘ Question 3:

### ğŸ”‘ Question 4:

### ğŸ”‘ Question 5:

---

## âœ¨ Final Reflections

> What did you learn from this project? What would you do differently next time? What did AI tools help you with the most?

âœï¸ *Your final thoughts here...*

---
